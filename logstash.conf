# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

# "instanceId"
# "requestQuery"
# "httpVersion"
# "receivedBytes"
# "sentBytes"
# "timeTaken"
# "sslEnabled"

input {
  beats {
    port => 5044
  }
}

filter {
  grok {
    match => ["source", "%{GREEDYDATA}\\(?<folder>[a-zA-Z0-9_-]+)\\(?<filename>[a-zA-Z0-9_-]+)\.csv"]
  }

  if "_grokparsefailure" in [tags] {
    mutate {
      remove_tag => [ "_grokparsefailure" ]
      add_field => { "[@metadata][logtype]" => "tag" }
    }
    grok {
      match => ["source", "%{GREEDYDATA}\\(?<folder>[a-zA-Z0-9_-]+)\\(?<filename>[a-zA-Z0-9_-]+)\.(?<suffix>[a-zA-Z0-9_-]+)\.csv"]
    }
  }
  else {
    mutate {
      add_field => { "[@metadata][logtype]" => "data" }
    }
  }

  if [@metadata][logtype] == "data" {
    csv {
      separator => ","
      columns => ["Id", "Timestamp", "PreciseTimeStamp", "ServicePrefix", "Region", "GatewayId", "Tenant", "Role", "RoleInstance", "ResourceId", "operationName", "time", "category", "instanceId", "clientIP", "clientPort", "httpMethod", "requestUri", "userAgent", "httpStatus", "httpVersion", "receivedBytes", "sentBytes", "timeTaken", "sslEnabled", "host", "requestQuery", "CACHE-HIT", "SERVER-ROUTED", "LOG-ID", "SERVER-STATUS", "RowKey", "__SourceEvent__", "__SourceMoniker__"]
      remove_field => ["PreciseTimeStamp", "ServicePrefix", "GatewayId", "Role", "RoleInstance", "operationName", "time", "category", "RowKey", "__SourceEvent__", "__SourceMoniker__"]
      remove_field => ["message", "offset", "tags", "prospector", "beat", "input"]
    }

    date {
      match => ["Timestamp", "ISO8601"]
      target => "@timestamp"
      remove_field => ["Timestamp"]
    }

    mutate {
      rename => {
        "[instanceId]" => "[data][instanceId]"
        "[httpVersion]" => "[data][httpVersion]"
        "[receivedBytes]" => "[data][receivedBytes]"
        "[sentBytes]" => "[data][sentBytes]"
        "[timeTaken]" => "[data][timeTaken]"
        "[sslEnabled]" => "[data][sslEnabled]"

        "[Region]" => "[out][Region]"
        "[Tenant]" => "[out][Tenant]"
        "[ResourceId]" => "[out][ResourceId]"
        "[source]" => "[out][source]"
        "[folder]" => "[out][folder]"
        "[filename]" => "[out][filename]"

        "[CACHE-HIT]" => "[AG][CACHE-HIT]"
        "[SERVER-ROUTED]" => "[AG][SERVER-ROUTED]"
        "[LOG-ID]" => "[AG][LOG-ID]"
        "[SERVER-STATUS]" => "[AG][SERVER-STATUS]"
      }

      # https://www.elastic.co/blog/little-logstash-lessons-part-using-grok-mutate-type-data
      convert => {
        "[Id]" => "integer"
        "[clientPort]" => "integer"
        "[data][receivedBytes]" => "integer"
        "[data][sentBytes]" => "integer"
        "[data][timeTaken]" => "float"
      }
    }
  }
  else {
    csv {
      separator => ","
      columns => ["Id", "value"]
      remove_field => ["message", "offset", "tags", "prospector", "beat", "input"]
    }

    mutate {
      rename => {
        "[value]" => "%{[suffix]}"
      }
    }
  }
}

output {
  if "_csvparsefailure" in [tags] or "_grokparsefailure" in [tags] {
    stdout {
      codec => rubydebug {
        metadata => true
      }
    }
  }

  if [@metadata][logtype] == "data" {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "%{[out][folder]}_%{[out][filename]}_data"
      document_id => "%{[Id]}"
    }
  }
  else {
    elasticsearch {
      hosts => ["http://localhost:9200"]
      index => "%{[folder]}_%{[filename]}_tag"
      document_id => "%{[Id]}"
    }
  }
}
