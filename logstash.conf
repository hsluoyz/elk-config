# Sample Logstash configuration for creating a simple
# Beats -> Logstash -> Elasticsearch pipeline.

# "instanceId"
# "requestQuery"
# "httpVersion"
# "receivedBytes"
# "sentBytes"
# "timeTaken"
# "sslEnabled"

input {
  beats {
    port => 5044
  }
}

filter {
  csv {
    separator => ","
    columns => ["Timestamp","PreciseTimeStamp","ServicePrefix","Region","GatewayId","Tenant","Role","RoleInstance","ResourceId","operationName","time","category","properties","RowKey","__SourceEvent__","__SourceMoniker__","timeStamp"]
    remove_field => ["message", "PreciseTimeStamp","ServicePrefix","GatewayId","Role","RoleInstance","operationName","time","category","RowKey","__SourceEvent__","__SourceMoniker__","timeStamp"]
    remove_field => ["offset", "host", "tags", "prospector", "beat", "input"]
  }

  date {
    match => ["Timestamp" , "ISO8601"]
    target => "@timestamp"
    remove_field => ["Timestamp"]
  }

  json {
    source => "properties"
    target => "data"
    remove_field => ["properties"]
  }

  mutate {
    rename => {
      "[data][clientIP]" => "[clientIP]"
      "[data][clientPort]" => "[clientPort]"
      "[data][httpMethod]" => "[method]"
      "[data][requestUri]" => "[uri]"
      "[data][userAgent]" => "[userAgent]"
      "[data][httpStatus]" => "[status]"
      "[data][host]" => "[server]"
    }
  }

  grok {
    match => ["source", "%{GREEDYDATA}\\%{USERNAME:folder}\\%{USERNAME:filename}\.csv"]
  }

  if "_jsonparsefailure" in [tags] {
    drop { }
  }
}

output {
  stdout { codec => rubydebug }
  elasticsearch {
    hosts => ["http://localhost:9200"]
    index => "%{[folder]}_%{[filename]}"
  }
}
